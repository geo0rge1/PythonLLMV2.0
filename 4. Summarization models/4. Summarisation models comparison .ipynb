{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c46312a0",
   "metadata": {},
   "source": [
    "# Домашнее задание №4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccdd9cb",
   "metadata": {},
   "source": [
    "1. Используя сайт HuggingFace, выберите задачу по обработке текстов на русском языке (например, классификация текста, генерация текста, ответы на вопросы и т.д.).\n",
    "\n",
    "\n",
    "2. Найдите подходящий датасет для выбранной задачи, используя ресурсы Kaggle или HuggingFace. В качестве альтернативы можно написать вручную или сгенерировать небольшой тестовый датасет. Данные вам будут нужны не для обучения моделей, а для тестирования.\n",
    "\n",
    "\n",
    "3. На HuggingFace выберите 3-5 различных предобученных моделей, которые могут быть применены для решения вашей задачи. Это могут быть как модели для русского языка, так и мультиязычные.\n",
    "\n",
    "\n",
    "4. Создайте ноутбук на Colab или Kaggle, в котором продемонстрируете применение каждой из выбранных моделей для решения вашей задачи.\n",
    "\n",
    "\n",
    "5. Проанализируйте и сравните результаты, полученные с помощью разных моделей.\n",
    "\n",
    "\n",
    "6. На основе проведенного анализа, выберите наиболее подходящую модель для решения вашей задачи. Обоснуйте свой выбор."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33212e6b",
   "metadata": {},
   "source": [
    "Для реализациия была выбрана задача суммаризации текстов.\n",
    "Тестовый датасет - https://www.kaggle.com/datasets/phoenix120/gazeta-summaries/data\n",
    "Модели для оценки:\n",
    "1. bartowski/Starling-LM-7B-beta-GGUF - квантизация 4Q_XS\n",
    "3. oblivious/ruGPT-3.5-13B-GGUF - Квантизация 4Q\n",
    "4. IlyaGusev/saiga_llama3_8b_gguf - Квантизация 4Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92171c06",
   "metadata": {},
   "source": [
    "**Импорт библиотек**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73c48fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d785e013",
   "metadata": {},
   "source": [
    "**Функция для чтения датасета(он в формате json строк)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b86b1187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gazeta_records(file_name, shuffle=False, sort_by_date=True):\n",
    "    assert shuffle != sort_by_date\n",
    "    records = []\n",
    "    with open(file_name, \"r\", encoding=\"utf-8-sig\") as r:\n",
    "        for line in r:\n",
    "            records.append(json.loads(line))\n",
    "    if sort_by_date:\n",
    "        records.sort(key=lambda x: x[\"date\"])\n",
    "    if shuffle:\n",
    "        random.shuffle(records)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa064ae",
   "metadata": {},
   "source": [
    "**Инференс GGUF моделей**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "472cf1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(model_path, text):\n",
    "    llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=4096,  # Context length to use\n",
    "    n_threads=4,  # Number of CPU threads to use\n",
    "    n_gpu_layers=10, # Number of model layers to offload to GPU\n",
    "    n_predict = -1,\n",
    "    temp = 0.8\n",
    "    )\n",
    "    generation_kwargs = {\n",
    "        \"max_tokens\": -1,\n",
    "        \"stop\":[\"<|end_of_turn|>\",\"GPT4 Correct User:\",\"GPT4 Correct Assistant:\",\"<|end_of_turn|>GPT4 Correct User:\"],\n",
    "        \"echo\":True, # Echo the prompt in the output\n",
    "        \"top_k\":40, # This is essentially greedy decoding, since the model will always return the highest-probability token. Set this value > 1 for sampling decoding\n",
    "        \"repeat_penalty\": 1.1,\n",
    "        \"min_p\": 0.05,\n",
    "        \"top_p\": 0.95,    \n",
    "    }\n",
    "\n",
    "    prompt = \"GPT4 Correct User: Кратко изложи текст на Русском языке: \" + text + \"<|end_of_turn|>GPT4 Correct Assistant:\"\n",
    "    res = llm(prompt, **generation_kwargs) # Res is a dictionary\n",
    "\n",
    "    summ_text = res[\"choices\"][0][\"text\"]\n",
    "    summ_text = re.sub(\".*(?=GPT4 Correct Assistant:)\", \"\", summ_text)\n",
    "    summ_text = re.sub(\"GPT4 Correct Assistant: \", \"\", summ_text)\n",
    "    return summ_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9859491f",
   "metadata": {},
   "source": [
    "**Небольшая предобработка текста для вычисления метрик**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "599dd229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(texts):\n",
    "    for text in texts:\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^а-яА-Я]', ' ', text)\n",
    "        text = re.sub(r'ё', 'е', text)\n",
    "        text = text.strip()\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a43a4a",
   "metadata": {},
   "source": [
    "**Подсчет метрик ROGUE-1, ROGUE-2, ROGUE-L - Случай Униграмм, биграмм и наиболее больших sequence'ов.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c15aa811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(hypothesises, references):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(hypothesises, references, avg=True)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e594b17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_path = r\"C:\\Users\\neytr\\Python Scripts\\Notebooks\\GGUF_models\\Starling-LM-7B-beta-IQ4_XS.gguf\"\n",
    "model_2_path = r\"C:\\Users\\neytr\\3. Binary Classification\\model-q4_K.gguf\"\n",
    "model_3_path = r\"C:\\Users\\neytr\\3. Binary Classification\\ruGPT-3.5-13B-Q4_0.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa539be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'date': '2020-09-02 14:05:31',\n",
       "  'url': 'https://www.gazeta.ru/tech/2020/09/02/13225772/bad_bluetooth.shtml',\n",
       "  'summary': 'Активированный в смартфоне Bluetooth может представлять угрозу для владельца устройства — хакеры могут использовать его, чтобы без особых усилий взломать гаджет, предупреждает эксперт. Эта технология, являющаяся «проклятием» для ИБ-специалистов, действительно обладает слабым уровнем защищенности, поэтому использовать ее рекомендуется как можно реже.',\n",
       "  'title': 'Названа опасность постоянно включенного Bluetooth ',\n",
       "  'text': 'Постоянно включенный Bluetooth на смартфоне грозит не только быстрой разрядкой аккумулятора, но и более серьезными проблемами, сообщил агентству « Прайм » доцент кафедры информатики РЭУ им. Плеханова Александр Тимофеев. По словам эксперта, хакеры могут использовать Bluetooth для взлома электронного устройства. «Возможность взлома Bluetooth может подвергнуть опасности любую информацию, хранящуюся на устройстве (фотографии, электронные письма, тексты). Кроме этого злоумышленник сможет получить контроль над устройством и отправлять на него нежелательные данные», — заявил Тимофеев. Эксперт рекомендует отключать Bluetooth, как только в нем исчезает необходимость, так как эта функция в активированном состоянии является «находкой для мошенников». Действительно, технология Bluetooth уже давно является головной болью для тех, кто занимается информационной безопасностью. Ни одна хакерская конференция в мире не проходит без презентации того, как с помощью Bluetooth можно взломать смартфон и украсть личные данные, или подслушать участника телефонного разговора. На одном из крупнейших подобных мероприятий, DEF CON, ежегодно проходящем в Лас-Вегасе, всем участникам настоятельно рекомендуют отключить Bluetooth во время посещения, сообщает Mashable. Известно, что с помощью Bluetooth злоумышленники могут получить контроль над незащищенными динамиками и колонками, заставив их проигрывать «опасные» звуки, которые при неудачном стечении обстоятельств могут обернуться для владельца гаджета частичной потерей слуха. Кроме того, Bluetooth позволяет хакерам перехватывать передаваемые данные и вносить в них свои изменения. Также Bluetooth может быть использован для слежки за покупателями и фиксированием их перемещений. Эта информация впоследствии передается рекламодателям, которые создают шопинг-портреты потребителей для более успешного рекламного таргетинга. «Мы не сталкивались с какой-либо атакой, которую можно было предотвратить, отключив Bluetooth на телефоне или отключив режим обнаружения. Для проведения большинства атак злоумышленникам нужно узнать MAC-адрес Bluetooth жертвы — то есть что-то, что однозначно идентифицирует целевой приемник Bluetooth. Например, это может быть номер телефона. Конечно, это легче сделать, если телефон можно обнаружить. Поэтому, если вы постоянно не используете Bluetooth, его лучше отключать», — рассказал «Газете.Ru» старший инженер-программист Avast Войтех Бочек. Постоянно включенный Bluetooth несет в себе значительную угрозу безопасности телефона и его владельца, соглашается руководитель группы системных инженеров по работе с партнерами в России Check Point Software Technologies Ltd. Сергей Забула. Мошенники непрерывно совершенствуют методы атак, и небольшой радиус распространения Bluetooth-сигнала уже не является для них проблемой. «Используя усилители, хакеры могут проникнуть в устройство пользователя, даже не запрашивая его разрешения и не зная секретного ключа соединения. Так, недавно обнаруженная французскими исследователями уязвимость в Bluetooth затронула миллиарды мобильных телефонов по всему миру. В результате атаки на протокол злоумышленник мог выдавать своё устройство за то, с которым уже спарено устройство жертвы», — пояснил Забула. Последствия атак с использованием Bluetooth могут быть разнообразны. Так, всего за несколько секунд мошенники могут подключиться к устройству пользователя, установить вредоносное ПО и в итоге украсть или удалить ценную информацию. Более того, через Bluetooth хакеры могут прослушивать звонки, устанавливать их переадресацию, а также отправлять вызовы и текстовые сообщения, что в свою очередь ведет к финансовым потерям жертвы. Также, с помощью Bluetooth соединения мошенники могут осуществить DoS-атаку (атака отказа в обслуживании), и полностью вывести телефон из строя. «Защититься от подобных атак можно с помощью постоянного обновления ОС. Однако основной способ — это использование Bluetooth на мобильном устройстве только по мере необходимости. Также для защиты мобильных устройств и критичных данных на них от различных видов угроз специалисты рекомендуют использовать решения класса Mobile Threat Defense (MTD)», — рекомендует эксперт.'},\n",
       " {'date': '2021-07-26 20:06:31',\n",
       "  'url': 'https://www.gazeta.ru/auto/2021/07/26_a_13802348.shtml',\n",
       "  'summary': 'Модели Lada заняли семь из десяти строчек рейтинга самых продаваемых подержанных автомобилей доступного сегмента, выяснили эксперты вторичного рынка. Лидеры среди более дорогих машин – седан Toyota Camry и внедорожник Land Cruiser. Люди не от хорошей жизни выбирают «Жигули», автопарк в России стремительно стареет и переломить эту ситуацию способен только рост доходов населения, считают экономисты.',\n",
       "  'title': 'Большинство автомобилистов России покупают старые Lada ',\n",
       "  'text': 'Покупатели самых доступных на вторичном рынке автомобилей (стоимостью до 1 млн рублей) чаще всего выбирают Lada Priora, «ВАЗ-2114» Samara, Granta и Kalina, свидетельствует данные классифайда «Авито Авто» за шесть месяцев 2021 года. Единственная иномарка в пятерке наиболее востребованных на вторичном рынке бюджетных машин — замыкающий ТОП-5 Ford Focus. Среди более дорогих автомобилей безоговорочно доминируют иностранные модели. В сегменте предложений ценой 1-2 млн рублей первое место занимает седан Toyota Camry. За ним следуют кроссоверы RAV4, Kia Sportage и Hyundai Creta. Если же ставка лота выше двух млн рублей, то в этом сегменте распространенность Toyota еще шире: первые три строчки рейтинга за Land Cruiser (всех поколений), Prado и более свежими и дорогими вариантами все той же Camry. Четвертое и пятое места – у BMW 5-series и Mercedes-Benz E-Class. Статистика свидетельствует, что общий автопарк России продолжает стареть: россияне перепродают друг другу старые машины, а пополнение дорог новым транспортом в последние годы идет невысокими темпами, несмотря на масштабную государственную поддержку отечественного автопрома. К примеру, только в 2020 году на льготные программы покупки российских автомобилей государство выделило более 45 млрд рублей. На вторичном рынке в самом доступном сегменте наибольшим спросом пользуются модели, снятые с производства либо ушедшие из России — как раз такие, как Lada Priora, Kalina, «ВАЗ-2107» и Ford Focus. Это свидетельствует о деградации автопарка в нашей стране. В период с 2016 по 2021 год на один купленный в России новый автомобиль приходится в среднем 4 подержанных, подсчитало аналитическое агентство «Автостат». Объемы рынка новых машин в последние годы находились в пределах 1,3-1,7 млн штук в год, а подержанных – 5,2-5,5 млн. При этом средний возраст легковой машины по состоянию на начало этого года практически достиг 14 лет, а доля машин старше 10 лет с 2011 до 2021 года выросла с 51% до 59%, свидетельствуют данные агентства. Автомобили, выставленные на продажу по объявлениям в интернете, в среднем оказываются еще старше. Средний возраст такой машины в 2021 году составил 14 лет и 10 месяцев, свидетельствуют данные исследования портала Drom.ru, проанализировавшего 1 млн объявлений о продаже машин. По сравнению с аналогичным периодом прошлого года этот показатель увеличился на пять месяцев. Структура парка легковых автомобилей в будущем неизбежно будет меняться, однако в среднесрочной перспективе расклад сил на вторичном рынке останется примерно таким же как сейчас, полагает независимый консультант по автопрому Сергей Бургазлиев. «В перспективе трех-четырех лет ситуация точно останется такой же, как есть сейчас. В своем сегменте автопарка автомобили «АвтоВАЗа» составляют не менее половины, – объясняет Бургазлиев в беседе с корреспондентом «Газеты.Ru», — К сожалению, для многих людей покупка подержанной «Лады» остается единственной возможностью так или иначе обновлять машину». Структура вторичного рынка будет меняться за счет подержанных автомобилей немецких и корейских марок в возрасте 7-10 лет, но изменения не будут взрывными, полагает он. Добиться существенного омоложения автопарка получится, если люди будут иметь финансовую возможность приобретать более дорогие и комфортные автомобили, считает доктор экономических наук Сергей Смирнов ( НИУ ВШЭ ). «Резкий рост продаж машин во-первых может быть связан с резким ростом доходов населения. И второе – нормальная инфраструктура. Дорожно-транспортное хозяйство, автомагистрали, платные дороги», — делится мнением в беседе с «Газетой.Ru» Смирнов. Вместе с тем, автомобильный рынок может очень радикально измениться вместе со сменой структуры потребления, считает экономист, – в пользу относительно дешевых и качественных такси, служб персональных водителей, общественного транспорта. Развитие этой отрасли тесно связано с техническим прогрессом и зависит от того, на сколько хватит запасов нефти, как пойдет развитие электромобилей, заметил ученый.'},\n",
       " {'date': '2021-08-21 09:27:44',\n",
       "  'url': 'https://www.gazeta.ru/business/2021/08/21/13899344.shtml',\n",
       "  'summary': 'Для развития Курильских островов правительство создаст там свободную налоговую зону — большинство инвесторов и предпринимателей освободят от НДС, налогов на транспорт, землю, имущество и прибыль. Льготы будут действовать минимум 10 лет. Ранее Трутнев заявлял, что с помощью офшора Курилы освоят за пять лет и тем самым «поставят точку» в территориальном воспросе.',\n",
       "  'title': 'Вице-премьер Трутнев рассказал о свободной налоговой зоне на Курилах ',\n",
       "  'text': 'Российские власти планируют создать на Курильских островах свободную налоговую зону на срок, как минимум, 10 лет. Об этом в интервью РБК рассказал полпред президента на Дальнем Востоке, вице-премьер Юрий Трутнев. «Это практически офшор. То есть, налогов на Курилах почти не останется», — заявил полпред. На время льгот предприятия не будут проверять контролирующие органы. Точные параметры будущего офшора, как отметил вице-премьер, сейчас обсуждаются в правительстве. Ранее в августе Трутнев уже заявлял, что с помощью свободной налоговой зоны власти намерены ускорить темпы освоения Курильских островов. Причем таким методом Москва не только хочет поспособствовать предпринимательству, но и дать сигнал соседним странам, прежде всего — Японии, претендующей на острова. Создание офшора, по словам полпреда, позволит освоить Курилы в течение пяти лет и «поставить точку» в вопросе о территориальной принадлежности островов. Согласно поручениям премьер-министра Михаила Мишустина, к 1 сентября кабмин должен представить план особого налогового режима для Курил, который будет включать снижение ставки обязательных платеже по страховым взносам на 7,6%, освобождение резидентов зоны от налогов на прибыль, имущество, НДС, земельного и транспортного налога. В конце июля Мишустин посещал Курилы и говорил, что от налогов будет освобождены большинство видов коммерческой деятельности. Однако налога на доходы физлиц будущие послабления не коснутся. Премьер отмечал, что курильская модель не станет подобием внутренних офшоров, которые создавались в стране в 1990-е годы (речь идет об Ингушетии, Калмыкии, Алтае) — тогда, по словам Мишустина, зоны создавались не для инвестиций в территорию, а для минимизации последствий «агрессивного налогообложения». В июле президент Владимир Путин заявлял, что Москва готовит «беспрецедентные предложения» по привлечению Токио к развитию Курил. Он не раскрывал детали, но говорил, что российская и японская стороны давно уже работают над повышением экономической привлекательности островов. Мишустин в ходе рабочей поездки на Дальний Восток подтверждал, что свободная налоговая зона должна привлечь японцев к инвестициям в коммерческое развитие Курил. Япония с середины 1940-х годов оспаривает территориальную принадлежность четырех южных островов Курильской гряды — Итурупа, Кунашира, Шикотана и Хамобаи. По итогам Второй мировой войны они стали территорией СССР, а сейчас входят в состав Сахалинской области. Токио же считает острова оккупированными северными территориями и относит их к префектуре Хоккайдо. Из-за курильского спора между Японией и Россией до сих пор официально не заключен мирный договор. Бывший японский премьер Синдзо Абэ планировал решить курильский вопрос: проводились переговоры и вырабатывались пути совместного освоения островов двумя странами. Однако Абэ не успел завершить процесс: диалогу помешали пандемия коронавирусной инфекции, а также принятые в прошлом году поправки в российскую Конституцию, которые не допускают даже разговоров об отчуждении части территорий страны. В 2020-м году Абэ ушел в отставку. Его преемник Ёсихидэ Суга в 2021 году заявлял, что готов продолжить переговоры с Москвой. Глава российского МИДа Сергей Лавров говорил, что «нервная реакция» Токио на развитие Курил российскими властями мешает переговорному процессу и последующему подписанию мирного договора. «Нам это непонятно, и мы очень жестко каждый раз реагируем, напоминая японцам, что это — неотъемлемая часть территории РФ, которая вошла в состав нашей страны по итогам Второй мировой войны, что закреплено в Уставе ООН », — подчеркивал Лавров.'},\n",
       " {'date': '2021-06-28 11:37:23',\n",
       "  'url': 'https://www.gazeta.ru/science/2021/06/28_a_13679336.shtml',\n",
       "  'summary': 'В Новой Зеландии благодаря тесту на COVID-19 врачи обнаружили в носу женщины фишку для игры, которую она вдохнула 37 лет назад. Все это время пациентка мучилась от проблем с дыханием, но не связывала их со случаем из детства. После операции она надеется, что сможет дышать нормально.',\n",
       "  'title': 'В Новой Зеландии врачи удалили из носа женщины игрушку, попавшую туда 37 лет назад ',\n",
       "  'text': 'Дети, засунувшие в ухо или нос деталь от игрушки — нередкие гости в кабинете врача. Однако со взрослыми пациентами, сохранившими в своем организме инородный предмет с самого детства, врачи встречаются куда реже. Мэри Маккарти из Новой Зеландии, 45-летняя сотрудница больничной кухни, основную часть своей жизни страдала от боли в правой ноздре, но не связывала ее с происшествием из детства. «Блошки» — настольная игра, в которой надо забросить маленькие пластиковые диски в коробку. Для этого на край диска надо надавить специальной пластиной и заставить его подпрыгнуть. Маккарти в детстве часто играла в «Блошки» с братьями и сестрами, и придумала себе еще одно развлечение — засовывала по диску в каждую ноздрю, а затем выдувала их. «Однажды я случайно вдохнула один диск вместо того, чтобы выдохнуть, и побоялась сказать маме, — вспоминает Маккарти, рассказывая свою историю новозеландскому изданию Stuff. — Помню, как я тогда испугалась и подумала — куда он делся?». Жаловаться Маккарти не любила, поэтому постаралась просто забыть об этом случае и жить дальше. «В течение многих лет у меня всегда были трудности с дыханием через нос, но я никогда не задумывалась об этом», — признается она. Но в 2020 году после мазка из носа на COVID-19 Маккарти столкнулась с сильной болью, а затем у нее стало постоянно течь из носа. Она обращалась к нескольким врачам, но те считали, что дело в хроническом заболевании пазух. «В моей жизни происходило много всего, поэтому я задвинула это на задний план», — говорит Маккарти. Маккарти живет с 22-летним сыном, страдающим тяжелой формой аутизма, и ей приходится уделять ему много времени. Поэтому следующий прием, на этот раз у частного специалиста, она отложила до августа, хотя симптомы мучали ее уже восемь месяцев. Но помощь понадобилась ей раньше. В один из дней, закончив работу, Маккарти тут же обратилась в отделение неотложной помощи госпиталя Крайстчерча, одной из ведущих новозеландских больниц. Во время осмотра врачи спросили ее, не засовывала ли она что-нибудь в нос. Маккарти вспомнила случай с «Блошками» из детства. Компьютерная томография подтвердила: маленький пластиковый диск все еще был в носу. Во время мазка, очевидно, он сдвинулся с места и привел к инфекции. Врачи попытались сразу же извлечь его, но вокруг образовались кальцификаты, так что пришлось ввести Маккарти наркоз и прооперировать. Диск с кальцификатами протолкнули по носовому ходу и извлекли через рот. «Когда я проснулась, то спросила — что это было? — рассказывает Маккарти. — А они сказали — то, что насмешило всю больницу. Маленький диск, который даже цвет не потерял. Вокруг него был кальцификат, и, видимо, поэтому мой нос немного искривился». Сейчас Маккарти восстанавливается после операции. Она с нетерпением ждет новой жизни, в которой у нее будет прямой, хорошо дышащий нос. Подобные случаи известны и в России. В 2020 году в Москве врачи вытащили из носа пациента монету, которая пробыла там более 50 лет. 59-летний мужчина обратился в поликлинику и пожаловался на то, что несколько месяцев не может дышать. Компьютерная томография показала, что в глубине правой ноздри оказались инородное тело каменистой плотности и предмет округлой формы. Также врачи выявили искривление носовой перегородки. Только после КТ пациент вспомнил, что в возрасте шести лет во время игры засунул однокопеечную монетку в нос, а потом вовсе о ней забыл. Как сообщила врач-оториноларинголог больницы имени Кончаловского Татьяна Михайлова , за 50 с лишним лет вокруг монеты в носу сформировался конгломерат каменистой плотности – ринолит. Врачи при помощи эндоскопической операции выполнили подслизистую коррекцию носовой перегородки пациента, удалив искривленную часть хряща, и извлекли инородное тело из полости носа. Вскоре мужчина вернулся домой.'},\n",
       " {'date': '2021-06-16 13:17:53',\n",
       "  'url': 'https://www.gazeta.ru/social/2021/06/16/13643954.shtml',\n",
       "  'summary': 'Правительства Москвы и Московской области объявили об обязательной вакцинации для отдельных категорий граждан от COVID-19. Согласно постановлению главного санитарного врача столицы Елены Андреевой, в общей сложности должны быть привиты как минимум 60% от общей численности работников сфер услуг, торговли и транспорта. Такое решение обосновывается неблагополучной эпидемической ситуацией. За прошедшие сутки количество новых случаев коронавирусной инфекции составило 13 397.',\n",
       "  'title': 'Главный санитарный врач Москвы ввела обязательную вакцинацию 60% сотрудников сферы услуг ',\n",
       "  'text': 'За прошедшие сутки в России подтверждено еще 13 397 новых случаев заражения коронавирусом. Из оперативной сводки оперштаба следует, что четвертый день подряд в стране ежедневный прирост не опускает ниже 13 тыс. больных. При этом показатель смертность продолжает расти — количество летальных исходов достигло 396 человек. Лидером по росту заболеваемости в регионах с большим отрывом остается Москве, где, согласно обновленным данным, за сутки COVID-19 диагностировали у 5 782 человек, что более чем на 1 тыс. меньше, чем днем ранее. Следом идут Подмосковье и Санкт-Петербург. Выздоровели за день 10 256 человек. В связи с возрастающим темпом распространения COVID-19 власти предпринимают новые меры по предотвращению развития пандемии. Так, главный санитарный врач Москвы Елена Андреева приняла постановление об обязательной вакцинации 60% от общей численности сотрудников сфер торговли, услуг, образования и транспорта столицы. Ответственность за обеспечение вакцинации для работников возложена на руководителей организаций и индивидуальных предпринимателей, контроль за ситуацией будет осуществлять московское правительство. Аналогичное решение о вакцинации для отдельных категорий граждан приняли власти Московской области. «Эпидемическая ситуация расценивается как неблагополучная и продолжает ухудшаться», — говорится в тексте документа, опубликованном Роспотребнадзором. Уточняется, что первый укол (или однокомпонентную вакцину) сотрудникам должны провести не позднее 15 июля, вторую — до 15 августа. Исключение составляют лишь те граждане, которые имеют противопоказания к вакцине по состоянию здоровья. Обязательная вакцинация относится в том числе к водителям такси и общественного транспорта, сотрудникам салонов красоты и заведений общественного питания, работникам образовательных учреждений, химчисток и других организаций, а также занятым на досуговых и развлекательных мероприятиях. «Мы просто обязаны сделать все, чтобы в самые короткие сроки провести массовую вакцинацию и остановить страшную болезнь. Прекратить гибель тысяч людей. Прошу понять правильно и поддержать это крайне тяжелое, непростое, но необходимое и ответственное решение», — добавил мэр Москвы Сергей Собянин. По словам Собянина, до 1 июля будет создана информационная система для контроля за вакцинацией. Пока не уточняется, как именно она будет работать. Математики из Санкт-Петербургского государственного университета утверждают , что при худшем развитии эпидемиологической обстановки в России средний суточный прирост новых случаев коронавируса может вырасти до 25 тыс. человек. Это более чем на 40% больше, чем регистрируется в России ежедневно в последние дни, но меньше максимальных показателей за все время (29 499 от декабря 2020-го). Такой вариант, как утверждают ученые, возможен ориентировочно в конце июня — на этот период придется пик. В ближайшей перспективе, по мнению аналитиков, также может быть зафиксирован рекордный показатель по количеству активных больных COVID-19. «Число активных больных составит 580 тыс., что даже выше январского пика. Напомню, в январе максимальное число активных случаев составило порядка 560 тыс. Однако количество новых случаев в январе было выше — около 29 тыс.», — добавил в беседе с газетой «Известия» заведующий кафедры математического моделирования энергетических систем СПбГУ Виктор Захаров. В то же время отмечается, что это наиболее пессимистичный прогноз, и при нем продлится пик заболеваемости недолго: согласно прогнозу математиков, на спад новая вспышка пойдет уже к началу августа — ожидается, что к этому времени суточный прирост уменьшится вдвое от тех показателей, которые возможны в период пиковых значений. Вместе с тем Захаров указал и на то, что возможен и четвертый пик, который будет схож с третьим. Избежать его удастся лишь при резком увеличении темпов вакцинации, утверждает руководитель Центра интеллектуальной логистики СПбГУ. В ином случае в дальнейшем, как он полагает, будет не только четвертый, но и пятый пик. В России в настоящий момент зафиксирован не только уханьский штамм коронавируса, но и индийский — причем в измененной, повторно мутировавшей форме. Об этом заявил директор НИЦ эпидемиологии и микробиологии имени Гамалеи Александр Гинцбург. Одновременно он добавил, что в лабораторных условиях отечественная вакцина «Спутник V» показывает высокие результаты в отношении индийского штамма. Это стало возможно благодаря тому, что «Спутник V» обеспечивает большой запас по концентрации антител. «Есть снижение 1,4-2,4 раза по сравнению с сывороткой, полученной от уханьских штаммов. Это совсем не критично», — приводит слова Гинцбурга «Интерфакс». Напомним, что первых носителей индийского штамма COVID-19 — «Дельта» — в России выявили в мае 2021 года. Согласно недавнему заявлению РИА Новости заведующего Сектором молекулярной эволюции ИППИ РАН профессор Георгия Базыкина, именно «Дельта» имеет потенциал стать доминирующей на территории России.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_records = read_gazeta_records(r\"C:\\Users\\neytr\\3. Binary Classification\\gazeta_test.jsonl\", shuffle=True, sort_by_date=False)[: 5]\n",
    "test_records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591faa0b",
   "metadata": {},
   "source": [
    "**Подсчет для Starling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edc7b080",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(test_records)):\n",
    "    hyps = []\n",
    "    hyps.append(model_inference(model_1_path, test_records[i][\"text\"]))\n",
    "    refs = []\n",
    "    refs.append(test_records[i][\"summary\"])\n",
    "hyps = clean_text(hyps)\n",
    "refs = clean_text(refs)\n",
    "result = evaluate_metrics(hyps, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50f96502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.2727272727272727,\n",
       "  'p': 0.11904761904761904,\n",
       "  'f': 0.16574585212295118},\n",
       " 'rouge-2': {'r': 0.07017543859649122,\n",
       "  'p': 0.027586206896551724,\n",
       "  'f': 0.0396039563449666},\n",
       " 'rouge-l': {'r': 0.23636363636363636,\n",
       "  'p': 0.10317460317460317,\n",
       "  'f': 0.14364640460913902}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9309a8",
   "metadata": {},
   "source": [
    "**Подсчет для Saiga**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36efed4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from C:\\Users\\neytr\\3. Binary Classification\\model-q4_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = saiga_llama3_8b\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128000\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = saiga_llama3_8b\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: PAD token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4685.30 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'saiga_llama3_8b', 'general.architecture': 'llama', 'llama.block_count': '32', 'llama.context_length': '8192', 'tokenizer.ggml.eos_token_id': '128009', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '500000.000000', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.padding_token_id': '128000', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: llama-3\n",
      "\n",
      "llama_print_timings:        load time =   24816.53 ms\n",
      "llama_print_timings:      sample time =     199.29 ms /   481 runs   (    0.41 ms per token,  2413.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   56225.28 ms /  1163 tokens (   48.35 ms per token,    20.68 tokens per second)\n",
      "llama_print_timings:        eval time =   58661.74 ms /   480 runs   (  122.21 ms per token,     8.18 tokens per second)\n",
      "llama_print_timings:       total time =  116241.59 ms /  1643 tokens\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from C:\\Users\\neytr\\3. Binary Classification\\model-q4_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv   1:                               general.name str              = saiga_llama3_8b\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128000\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = saiga_llama3_8b\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: PAD token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4685.30 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'saiga_llama3_8b', 'general.architecture': 'llama', 'llama.block_count': '32', 'llama.context_length': '8192', 'tokenizer.ggml.eos_token_id': '128009', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '500000.000000', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.padding_token_id': '128000', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: llama-3\n",
      "\n",
      "llama_print_timings:        load time =   22308.81 ms\n",
      "llama_print_timings:      sample time =     204.09 ms /   504 runs   (    0.40 ms per token,  2469.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =   56044.17 ms /  1231 tokens (   45.53 ms per token,    21.96 tokens per second)\n",
      "llama_print_timings:        eval time =   58976.68 ms /   503 runs   (  117.25 ms per token,     8.53 tokens per second)\n",
      "llama_print_timings:       total time =  116410.84 ms /  1734 tokens\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from C:\\Users\\neytr\\3. Binary Classification\\model-q4_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = saiga_llama3_8b\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128000\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = saiga_llama3_8b\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: PAD token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4685.30 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'saiga_llama3_8b', 'general.architecture': 'llama', 'llama.block_count': '32', 'llama.context_length': '8192', 'tokenizer.ggml.eos_token_id': '128009', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '500000.000000', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.padding_token_id': '128000', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: llama-3\n",
      "\n",
      "llama_print_timings:        load time =   23067.53 ms\n",
      "llama_print_timings:      sample time =     295.75 ms /   722 runs   (    0.41 ms per token,  2441.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =   52947.67 ms /  1154 tokens (   45.88 ms per token,    21.80 tokens per second)\n",
      "llama_print_timings:        eval time =   84871.60 ms /   721 runs   (  117.71 ms per token,     8.50 tokens per second)\n",
      "llama_print_timings:       total time =  139993.72 ms /  1875 tokens\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from C:\\Users\\neytr\\3. Binary Classification\\model-q4_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = saiga_llama3_8b\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128000\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = saiga_llama3_8b\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: PAD token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4685.30 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'saiga_llama3_8b', 'general.architecture': 'llama', 'llama.block_count': '32', 'llama.context_length': '8192', 'tokenizer.ggml.eos_token_id': '128009', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '500000.000000', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.padding_token_id': '128000', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: llama-3\n",
      "\n",
      "llama_print_timings:        load time =   23103.58 ms\n",
      "llama_print_timings:      sample time =     245.44 ms /   597 runs   (    0.41 ms per token,  2432.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =   59605.68 ms /  1205 tokens (   49.47 ms per token,    20.22 tokens per second)\n",
      "llama_print_timings:        eval time =   71271.34 ms /   596 runs   (  119.58 ms per token,     8.36 tokens per second)\n",
      "llama_print_timings:       total time =  132618.54 ms /  1801 tokens\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from C:\\Users\\neytr\\3. Binary Classification\\model-q4_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = saiga_llama3_8b\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128000\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = saiga_llama3_8b\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: PAD token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4685.30 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'saiga_llama3_8b', 'general.architecture': 'llama', 'llama.block_count': '32', 'llama.context_length': '8192', 'tokenizer.ggml.eos_token_id': '128009', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '500000.000000', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.padding_token_id': '128000', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: llama-3\n",
      "\n",
      "llama_print_timings:        load time =   21967.06 ms\n",
      "llama_print_timings:      sample time =     205.03 ms /   497 runs   (    0.41 ms per token,  2424.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =   67421.68 ms /  1488 tokens (   45.31 ms per token,    22.07 tokens per second)\n",
      "llama_print_timings:        eval time =   60378.34 ms /   496 runs   (  121.73 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:       total time =  129211.38 ms /  1984 tokens\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_records)):\n",
    "    hyps = []\n",
    "    hyps.append(model_inference(model_2_path, test_records[i][\"text\"]))\n",
    "    refs = []\n",
    "    refs.append(test_records[i][\"summary\"])\n",
    "hyps = clean_text(hyps)\n",
    "refs = clean_text(refs)\n",
    "result = evaluate_metrics(hyps, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dcc6898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.32727272727272727,\n",
       "  'p': 0.09782608695652174,\n",
       "  'f': 0.15062761151940626},\n",
       " 'rouge-2': {'r': 0.07017543859649122,\n",
       "  'p': 0.017699115044247787,\n",
       "  'f': 0.02826854801982831},\n",
       " 'rouge-l': {'r': 0.3090909090909091,\n",
       "  'p': 0.09239130434782608,\n",
       "  'f': 0.14225941068258618}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215d4a34",
   "metadata": {},
   "source": [
    "**Подсчет для RuGPT3.5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f04047b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 18 key-value pairs and 485 tensors from C:\\Users\\neytr\\3. Binary Classification\\ruGPT-3.5-13B-Q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gpt2\n",
      "llama_model_loader: - kv   1:                               general.name str              = ruGPT-3.5-13B\n",
      "llama_model_loader: - kv   2:                           gpt2.block_count u32              = 40\n",
      "llama_model_loader: - kv   3:                        gpt2.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                      gpt2.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   5:                   gpt2.feed_forward_length u32              = 20480\n",
      "llama_model_loader: - kv   6:                  gpt2.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   7:          gpt2.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   8:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv   9:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  10:                      tokenizer.ggml.tokens arr[str,50272]   = [\"<pad>\", \"<|endoftext|>\", \"<s>\", \"</...\n",
      "llama_model_loader: - kv  11:                  tokenizer.ggml.token_type arr[i32,50272]   = [3, 3, 3, 3, 1, 3, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.merges arr[str,49995]   = [\"Ġ Ð\", \"Ð ¾\", \"Ð µ\", \"Ð °\", ...\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 3\n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  16:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  322 tensors\n",
      "llama_model_loader: - type q4_0:  162 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: mismatch in special tokens definition ( 21/50272 vs 20/50272 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gpt2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 50272\n",
      "llm_load_print_meta: n_merges         = 49995\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 20480\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = -1\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.11 B\n",
      "llm_load_print_meta: model size       = 6.94 GiB (4.55 BPW) \n",
      "llm_load_print_meta: general.name     = ruGPT-3.5-13B\n",
      "llm_load_print_meta: BOS token        = 2 '<s>'\n",
      "llm_load_print_meta: EOS token        = 3 '</s>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 134 'Ä'\n",
      "llm_load_print_meta: EOT token        = 1 '<|endoftext|>'\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size =  7105.26 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  3200.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3200.00 MiB, K (f16): 1600.00 MiB, V (f16): 1600.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   348.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1489\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'gpt2.embedding_length': '5120', 'general.name': 'ruGPT-3.5-13B', 'general.architecture': 'gpt2', 'gpt2.block_count': '40', 'tokenizer.ggml.eos_token_id': '3', 'general.file_type': '2', 'gpt2.context_length': '2048', 'gpt2.feed_forward_length': '20480', 'gpt2.attention.head_count': '40', 'gpt2.attention.layer_norm_epsilon': '0.000010', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'false'}\n",
      "Using fallback chat format: llama-2\n",
      "\n",
      "llama_print_timings:        load time =   60012.45 ms\n",
      "llama_print_timings:      sample time =      89.43 ms /   483 runs   (    0.19 ms per token,  5400.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =  102736.95 ms /   871 tokens (  117.95 ms per token,     8.48 tokens per second)\n",
      "llama_print_timings:        eval time =  133242.42 ms /   482 runs   (  276.44 ms per token,     3.62 tokens per second)\n",
      "llama_print_timings:       total time =  236773.98 ms /  1353 tokens\n",
      "llama_model_loader: loaded meta data with 18 key-value pairs and 485 tensors from C:\\Users\\neytr\\3. Binary Classification\\ruGPT-3.5-13B-Q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gpt2\n",
      "llama_model_loader: - kv   1:                               general.name str              = ruGPT-3.5-13B\n",
      "llama_model_loader: - kv   2:                           gpt2.block_count u32              = 40\n",
      "llama_model_loader: - kv   3:                        gpt2.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                      gpt2.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   5:                   gpt2.feed_forward_length u32              = 20480\n",
      "llama_model_loader: - kv   6:                  gpt2.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   7:          gpt2.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   8:                          general.file_type u32              = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv   9:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  10:                      tokenizer.ggml.tokens arr[str,50272]   = [\"<pad>\", \"<|endoftext|>\", \"<s>\", \"</...\n",
      "llama_model_loader: - kv  11:                  tokenizer.ggml.token_type arr[i32,50272]   = [3, 3, 3, 3, 1, 3, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.merges arr[str,49995]   = [\"Ġ Ð\", \"Ð ¾\", \"Ð µ\", \"Ð °\", ...\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 3\n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  16:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  322 tensors\n",
      "llama_model_loader: - type q4_0:  162 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: mismatch in special tokens definition ( 21/50272 vs 20/50272 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gpt2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 50272\n",
      "llm_load_print_meta: n_merges         = 49995\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 20480\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = -1\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.11 B\n",
      "llm_load_print_meta: model size       = 6.94 GiB (4.55 BPW) \n",
      "llm_load_print_meta: general.name     = ruGPT-3.5-13B\n",
      "llm_load_print_meta: BOS token        = 2 '<s>'\n",
      "llm_load_print_meta: EOS token        = 3 '</s>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 134 'Ä'\n",
      "llm_load_print_meta: EOT token        = 1 '<|endoftext|>'\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size =  7105.26 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  3200.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3200.00 MiB, K (f16): 1600.00 MiB, V (f16): 1600.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   348.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1489\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'gpt2.embedding_length': '5120', 'general.name': 'ruGPT-3.5-13B', 'general.architecture': 'gpt2', 'gpt2.block_count': '40', 'tokenizer.ggml.eos_token_id': '3', 'general.file_type': '2', 'gpt2.context_length': '2048', 'gpt2.feed_forward_length': '20480', 'gpt2.attention.head_count': '40', 'gpt2.attention.layer_norm_epsilon': '0.000010', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'false'}\n",
      "Using fallback chat format: llama-2\n",
      "\n",
      "llama_print_timings:        load time =   58968.21 ms\n",
      "llama_print_timings:      sample time =     124.08 ms /   675 runs   (    0.18 ms per token,  5439.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =  107970.06 ms /   921 tokens (  117.23 ms per token,     8.53 tokens per second)\n",
      "llama_print_timings:        eval time =  181902.83 ms /   674 runs   (  269.89 ms per token,     3.71 tokens per second)\n",
      "llama_print_timings:       total time =  291096.36 ms /  1595 tokens\n",
      "llama_model_loader: loaded meta data with 18 key-value pairs and 485 tensors from C:\\Users\\neytr\\3. Binary Classification\\ruGPT-3.5-13B-Q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gpt2\n",
      "llama_model_loader: - kv   1:                               general.name str              = ruGPT-3.5-13B\n",
      "llama_model_loader: - kv   2:                           gpt2.block_count u32              = 40\n",
      "llama_model_loader: - kv   3:                        gpt2.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                      gpt2.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   5:                   gpt2.feed_forward_length u32              = 20480\n",
      "llama_model_loader: - kv   6:                  gpt2.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   7:          gpt2.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   8:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv   9:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  10:                      tokenizer.ggml.tokens arr[str,50272]   = [\"<pad>\", \"<|endoftext|>\", \"<s>\", \"</...\n",
      "llama_model_loader: - kv  11:                  tokenizer.ggml.token_type arr[i32,50272]   = [3, 3, 3, 3, 1, 3, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.merges arr[str,49995]   = [\"Ġ Ð\", \"Ð ¾\", \"Ð µ\", \"Ð °\", ...\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 3\n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  16:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  322 tensors\n",
      "llama_model_loader: - type q4_0:  162 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: mismatch in special tokens definition ( 21/50272 vs 20/50272 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gpt2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 50272\n",
      "llm_load_print_meta: n_merges         = 49995\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 20480\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = -1\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.11 B\n",
      "llm_load_print_meta: model size       = 6.94 GiB (4.55 BPW) \n",
      "llm_load_print_meta: general.name     = ruGPT-3.5-13B\n",
      "llm_load_print_meta: BOS token        = 2 '<s>'\n",
      "llm_load_print_meta: EOS token        = 3 '</s>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 134 'Ä'\n",
      "llm_load_print_meta: EOT token        = 1 '<|endoftext|>'\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size =  7105.26 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  3200.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3200.00 MiB, K (f16): 1600.00 MiB, V (f16): 1600.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   348.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1489\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'gpt2.embedding_length': '5120', 'general.name': 'ruGPT-3.5-13B', 'general.architecture': 'gpt2', 'gpt2.block_count': '40', 'tokenizer.ggml.eos_token_id': '3', 'general.file_type': '2', 'gpt2.context_length': '2048', 'gpt2.feed_forward_length': '20480', 'gpt2.attention.head_count': '40', 'gpt2.attention.layer_norm_epsilon': '0.000010', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'false'}\n",
      "Using fallback chat format: llama-2\n",
      "\n",
      "llama_print_timings:        load time =   63287.59 ms\n",
      "llama_print_timings:      sample time =      37.31 ms /   204 runs   (    0.18 ms per token,  5467.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =  101230.43 ms /   805 tokens (  125.75 ms per token,     7.95 tokens per second)\n",
      "llama_print_timings:        eval time =   52602.45 ms /   203 runs   (  259.13 ms per token,     3.86 tokens per second)\n",
      "llama_print_timings:       total time =  154097.46 ms /  1008 tokens\n",
      "llama_model_loader: loaded meta data with 18 key-value pairs and 485 tensors from C:\\Users\\neytr\\3. Binary Classification\\ruGPT-3.5-13B-Q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gpt2\n",
      "llama_model_loader: - kv   1:                               general.name str              = ruGPT-3.5-13B\n",
      "llama_model_loader: - kv   2:                           gpt2.block_count u32              = 40\n",
      "llama_model_loader: - kv   3:                        gpt2.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                      gpt2.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   5:                   gpt2.feed_forward_length u32              = 20480\n",
      "llama_model_loader: - kv   6:                  gpt2.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   7:          gpt2.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   8:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv   9:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  10:                      tokenizer.ggml.tokens arr[str,50272]   = [\"<pad>\", \"<|endoftext|>\", \"<s>\", \"</...\n",
      "llama_model_loader: - kv  11:                  tokenizer.ggml.token_type arr[i32,50272]   = [3, 3, 3, 3, 1, 3, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.merges arr[str,49995]   = [\"Ġ Ð\", \"Ð ¾\", \"Ð µ\", \"Ð °\", ...\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 3\n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  16:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  322 tensors\n",
      "llama_model_loader: - type q4_0:  162 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: mismatch in special tokens definition ( 21/50272 vs 20/50272 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gpt2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 50272\n",
      "llm_load_print_meta: n_merges         = 49995\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 20480\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = -1\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.11 B\n",
      "llm_load_print_meta: model size       = 6.94 GiB (4.55 BPW) \n",
      "llm_load_print_meta: general.name     = ruGPT-3.5-13B\n",
      "llm_load_print_meta: BOS token        = 2 '<s>'\n",
      "llm_load_print_meta: EOS token        = 3 '</s>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 134 'Ä'\n",
      "llm_load_print_meta: EOT token        = 1 '<|endoftext|>'\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size =  7105.26 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  3200.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3200.00 MiB, K (f16): 1600.00 MiB, V (f16): 1600.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   348.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1489\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'gpt2.embedding_length': '5120', 'general.name': 'ruGPT-3.5-13B', 'general.architecture': 'gpt2', 'gpt2.block_count': '40', 'tokenizer.ggml.eos_token_id': '3', 'general.file_type': '2', 'gpt2.context_length': '2048', 'gpt2.feed_forward_length': '20480', 'gpt2.attention.head_count': '40', 'gpt2.attention.layer_norm_epsilon': '0.000010', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'false'}\n",
      "Using fallback chat format: llama-2\n",
      "\n",
      "llama_print_timings:        load time =   60037.42 ms\n",
      "llama_print_timings:      sample time =      10.45 ms /    58 runs   (    0.18 ms per token,  5550.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =  110169.39 ms /   928 tokens (  118.72 ms per token,     8.42 tokens per second)\n",
      "llama_print_timings:        eval time =   14496.89 ms /    57 runs   (  254.33 ms per token,     3.93 tokens per second)\n",
      "llama_print_timings:       total time =  124731.46 ms /   985 tokens\n",
      "llama_model_loader: loaded meta data with 18 key-value pairs and 485 tensors from C:\\Users\\neytr\\3. Binary Classification\\ruGPT-3.5-13B-Q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gpt2\n",
      "llama_model_loader: - kv   1:                               general.name str              = ruGPT-3.5-13B\n",
      "llama_model_loader: - kv   2:                           gpt2.block_count u32              = 40\n",
      "llama_model_loader: - kv   3:                        gpt2.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                      gpt2.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   5:                   gpt2.feed_forward_length u32              = 20480\n",
      "llama_model_loader: - kv   6:                  gpt2.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   7:          gpt2.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   8:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv   9:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  10:                      tokenizer.ggml.tokens arr[str,50272]   = [\"<pad>\", \"<|endoftext|>\", \"<s>\", \"</...\n",
      "llama_model_loader: - kv  11:                  tokenizer.ggml.token_type arr[i32,50272]   = [3, 3, 3, 3, 1, 3, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.merges arr[str,49995]   = [\"Ġ Ð\", \"Ð ¾\", \"Ð µ\", \"Ð °\", ...\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 3\n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  16:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  322 tensors\n",
      "llama_model_loader: - type q4_0:  162 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: mismatch in special tokens definition ( 21/50272 vs 20/50272 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gpt2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 50272\n",
      "llm_load_print_meta: n_merges         = 49995\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 20480\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = -1\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_load_print_meta: model params     = 13.11 B\n",
      "llm_load_print_meta: model size       = 6.94 GiB (4.55 BPW) \n",
      "llm_load_print_meta: general.name     = ruGPT-3.5-13B\n",
      "llm_load_print_meta: BOS token        = 2 '<s>'\n",
      "llm_load_print_meta: EOS token        = 3 '</s>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 134 'Ä'\n",
      "llm_load_print_meta: EOT token        = 1 '<|endoftext|>'\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size =  7105.26 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  3200.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3200.00 MiB, K (f16): 1600.00 MiB, V (f16): 1600.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   348.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1489\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'gpt2.embedding_length': '5120', 'general.name': 'ruGPT-3.5-13B', 'general.architecture': 'gpt2', 'gpt2.block_count': '40', 'tokenizer.ggml.eos_token_id': '3', 'general.file_type': '2', 'gpt2.context_length': '2048', 'gpt2.feed_forward_length': '20480', 'gpt2.attention.head_count': '40', 'gpt2.attention.layer_norm_epsilon': '0.000010', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'false'}\n",
      "Using fallback chat format: llama-2\n",
      "\n",
      "llama_print_timings:        load time =   60096.49 ms\n",
      "llama_print_timings:      sample time =     135.51 ms /   735 runs   (    0.18 ms per token,  5423.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =  127810.70 ms /  1072 tokens (  119.23 ms per token,     8.39 tokens per second)\n",
      "llama_print_timings:        eval time =  202994.99 ms /   734 runs   (  276.56 ms per token,     3.62 tokens per second)\n",
      "llama_print_timings:       total time =  332254.26 ms /  1806 tokens\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_records)):\n",
    "    hyps = []\n",
    "    hyps.append(model_inference(model_3_path, test_records[i][\"text\"]))\n",
    "    refs = []\n",
    "    refs.append(test_records[i][\"summary\"])\n",
    "hyps = clean_text(hyps)\n",
    "refs = clean_text(refs)\n",
    "result = evaluate_metrics(hyps, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6ff9786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.6545454545454545,\n",
       "  'p': 0.09944751381215469,\n",
       "  'f': 0.17266186821363058},\n",
       " 'rouge-2': {'r': 0.2807017543859649,\n",
       "  'p': 0.033402922755741124,\n",
       "  'f': 0.05970149063662569},\n",
       " 'rouge-l': {'r': 0.6, 'p': 0.09116022099447514, 'f': 0.15827337900499747}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f330ca1",
   "metadata": {},
   "source": [
    "**Результат**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acec80f",
   "metadata": {},
   "source": [
    "Все три модели показывают не лучший результат, но наиболее хороший результат, на удивление у ruGPT3.5.\n",
    "\n",
    "1. ruGPT3.5\n",
    "2. Starling\n",
    "3. Saiga\n",
    "\n",
    "Для более точного определения следует еще сделать:\n",
    "1. Лемматизация/стемминг текста для более точно сравнения слов в гипотезе и референсе, а так же избавление текста от шума.\n",
    "2. Подсчет F1-score'а - Для этого помимо ROGUE score'а (Recall), следует еще посчитать BLEU (Precision) и потом по формуле F1 = 2 * (Bleu * Rouge) / (Bleu + Rouge) расчитать F1-score (возможно я не совсем разобрался в библиотеке rogue и там уже посчитана F1-score в 'F').\n",
    "3. Использовать полный датасет, а не такую маленькую выборку.\n",
    "4. Подобрать более хороший промт, добавить системный промт."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cf1f14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
